## Questions for the month of May
---
### 01/05/22
***One of these shouldn't be here***

Here are four different techniques commonly used in machine learning.

Although they are all related somehow, one of them is different from the rest. Your goal is to determine which of the following doesn't belong on this list.
**Can you select the odd one out?**

- [ ] Expectation-Maximization
- [x] PCA
- [ ] DBSCAN
- [ ] K-means
---
### 02/05/22
***The bankruptcy story***

Suzanne wants to build an algorithm to predict whether a company is about to declare bankruptcy over the next few months. She has access to a labeled dataset with detailed financial information from thousands of companies, including those that have declared bankruptcy over the last 100 years. Suzanne has some ideas but would love to hear what you think.

**Understanding that there are many ways to approach a problem, what would be your first recommendation to Suzanne?**
- [ ] The best way to approach this problem is with Supervised Learning by using a regression algorithm.
- [x] The best way to approach this problem is with Supervised Learning by using a classification algorithm.
- [ ] The best way to approach this problem is with Unsupervised Learning by using a clustering algorithm.
- [ ] The best way to approach this problem is with Reinforcement Learning.
---
### 03/05/22
***A batch of rotated pictures***

After looking at the last batch of images, the problem was apparent:

Customers were taking pictures and sending them with different degrees of rotation. The Convolutional Neural Network that Jessica built wasn't ready to handle this. She knew she needed to do something about it. A couple of meetings later, Jessica knew what the right solution was. It took some time for the team to agree, but they had a plan now.

**Which of the following approaches could Jessica have proposed?**

- [ ] Extending the pipeline with a data preprocessing step to properly rotate every image coming from the customer before giving the data to the model.
- [ ] Extending the model with a layer capable of rotating the data to the correct position.
- [ ] Extending the training data with samples of images rotated across the full 360-degree spectrum to build some rotation invariability into the model.
- [ ] Configuring the network correctly since Convolutional Neural Networks are translation and rotation invariant and should handle these images correctly.
---
### 04/05/22
***Alex's model is not doing well***

Alex is a Machine Learning Engineer working for a new photo-sharing startup.

His team started building a model to predict the likeability of every new image posted on the platform. They collected some data and built a simple classification model.*Unfortunately*, Alex quickly realizes that the model doesn't perform well. He notices that the training error is not as low as expected.

**What do you think is happening with Alex's model?**

- [x] It's very likely that the model suffers from high bias and is underfitting. This usually happens when the model is not complex enough and can't capture the relationship between input and output variables.
- [ ] It's very likely that the model suffers from low bias and is underfitting. This usually happens when the model is not complex enough and can't capture the relationship between input and output variables.
- [ ] It's very likely that the model suffers from high variance and is overfitting. This usually happens when the model is too complex and captures the noise of the data.
- [ ] It's very likely that the model suffers from low variance and is overfitting. This usually happens when the model is too complex and captures the noise of the data.
---
### 05/05/22
***Behind Gradient Descent***

It's 2030, and neural networks are taught at high schools worldwide.

It makes sense. Few subjects are as impactful to society as machine learning, so it's only appropriate that schools get students onboard from a very early age. Lillian spent a long time learning about gradient descent and how it's an optimization algorithm frequently used in machine learning applications. This is Lillian's last exam. The first question asks her to describe in a few words how gradient descent works.

**Which of the following statements is a sensible description of how the algorithm works?**

- [ ] Gradient descent identifies the minimum loss and adjusts every parameter proportionally to this loss.
- [ ] Gradient descent searches every possible combination of parameters to find the optimal loss.
- [x] Gradient descent identifies the slope in all directions and adjusts the parameters to move them in the direction of the negative slope.
- [ ] Gradient descent identifies the slope in all directions and adjusts the parameters to move them in the direction of the slope.
---
### 06/06/22
***A recommendation for Adrienne***

Kaggle looked like the perfect opportunity for Adrienne to start practicing machine learning.

She went online and started listening to the conversations about popular Kagglers. One particular topic caught her attention: They kept discussing different ways to create ensembles. Adrienne knew that ensemble learning is a powerful technique where you combine the decisions from multiple models to improve the overall performance. She had never used ensembles before, so she decided this was the place to start.

**Which of the following are valid ensemble techniques that Adrienne could study?**

- [x] Max Voting: Multiple models make predictions for each sample. The final prediction is the one produced by the majority of the models.
- [x] Weighted Voting: Multiple models make predictions for each sample, and each model is assigned a different weight. The final prediction considers the importance of the model in determining the final vote.
- [x] Simple Averaging: Multiple models make predictions for each sample. The final prediction is the average of all of those predictions.
- [x] Weighted Averaging: Multiple models make predictions for each sample, and each model is assigned a different weight. The final prediction is the average of all of those predictions, considering the importance of each model. 
---
### 07/05/22
***Sometimes, small is better***

Fynn is new to a team working on a neural network model. Unfortunately, they haven't been happy with the results so far. Fynn thinks that he found the problem: they chose a batch size as large as it fits into the GPU memory. His colleagues believe this is the right approach, but Fynn believes a smaller batch size will be better.

**What would be good arguments to support Fynn's suspicion?**

- [ ] A smaller batch size is more computationally effective.
- [x] A smaller batch size reduces overfitting because it increases the noise in the training process.
- [ ] A smaller batch size reduces overfitting because it increases the noise in the training process.
- [x] A smaller batch size can improve the generalization of the model.
---
### 08/05/22
***Reese's baseline***

Starting with a simple baseline is a great way to approach a new problem.
Reese knew that, and her go-to has always been a simple Linear Regression, probably one of the most popular algorithms in statistics and machine learning. But Reese knows that for Linear Regression to work, she must consider several assumptions about the problem.

**Which of the following are some of the assumptions that Reese should make for Linear Regression to be a good candidate for her baseline?**

- [x] The relationship between the features in the data and the target variable must be linear.
- [ ] The features in the data are highly correlated between them.
- [x] The features in the data and the target variable are not noisy.
- [ ] There must not be more than two relevant features plus the target variable.
---
### 09/05/22

***Migrating to PyTorch Lightning***

Many of the old team members have left Layla's company, forcing them to start building a new team.

They have been hiring from local universities, and most new hires brought a lot of experience in PyTorch Lightning. Unfortunately for Layla's company, their main product uses TensorFlow.

After some discussions, Layla's team decided to migrate their model to PyTorch Lightning. This change, however, will not come without making some concessions.

**Which of the following are some of the downsides of this decision?**
- [ ] The team will lose the ability to deploy the model in TPUs (Tensor Processing Units), limiting them to GPUs and CPUs.
- [ ] The team won't be able to use tools like TensorBoard during the training process, so they will need to find an equivalent tool compatible with PyTorch Lightning.
- [x] The team will have to invest time to migrate the deployment process of their model from TensorFlow Serving to something like TorchServe or PyTorch Live.
- [x] Migrating the existing codebase to PyTorch Lightning could introduce unforeseen problems that could cause issues with the new model.
---
### 10/05/22

***The brains behind transformers***

It took some time, but Kinsley finished replacing her old model based on a Long Short-Term Memory (LSTM) network with a new version using Transformers.

The results of the new model were impressive. The whole team was thrilled with Kinsley's work, and the company organized an internal session for Kinsley to bring everyone up to speed. After finishing her speech, a coworker asked Kinsley a question:

**Which company invented the Transformer architecture?**

- [ ] Hugging Face
- [ ] OpenAI
- [x] Google
- [ ] Allen Institute of AI
---
### 14/05/22

***Depth perception***
Richard finally got a job as a self-driving car engineer!

His first task is to help the car perceive depth using the onboard cameras. He wants to start with an overview of the different approaches he can use to estimate the distance to every pixel in the image. Before diving into the existing techniques, Richard has to think about the different ways he can capture pictures.

**Which of the following mechanisms do you think Richard can use to estimate depth?**

- [x] An image from a single camera.
- [x] A sequence of images from a single camera.
- [x] A pair of images from a stereo camera.
- [ ] Cameras are 2D sensors, so Richard can't use them to estimate depth. 
---
## 15/05/22
***The benefits of the Huber loss***

The Huber loss is a popular loss function used for regression problems in machine learning.

Here is the formula. Take a second and look at it. The formula may look complex, but there are two things you need to know about the Huber loss. First, it behaves like a square function for values smaller than a parameter δ (similar to MSE.) Second, it acts as the absolute function for larger values (similar to MAE.) In essence, the Huber loss is a combination of two other popular loss functions: Mean Squared Error (MSE) and Mean Absolute Error (MAE.)

**What are the benefits of combining these two functions?**

- [ ] It adds an additional hyperparameter δ which helps tune the model.
- [x] It is more robust against large outliers than MSE.
- [x] It is smooth around 0 helping the training converge better.
- [x] It is continuous and differentiable.
---
### 16/05/22
***Climbing a hill***

Gabriela wanted her friend to grow an appreciation for the outdoors, so they started meeting every Saturday and going for a hike together. And what better way to spend their time than starting a discussion about hill climbing and how it relates to their day-to-day work. It turns out that hill climbing is an optimization algorithm that attempts to find a better solution by making incremental changes until it doesn't see further improvements. Her friend couldn't help but notice how similar to gradient descent the process was, but Gabriela knew there were a few critical differences between them.

**Can you select every correct statement from the following comparison list?**

- [ ] Hill climbing is a general optimization algorithm, but gradient descent is only used to optimize neural networks.
- [ ] Unlike gradient descent, hill climbing can return an optimal solution even if it's interrupted at any time before it ends.
- [x] Gradient descent is usually more efficient than hill climbing, but there are fewer problems we can tackle with gradient descent.
- [x] Both hill climbing and gradient descent can find optimal solutions for convex problems.
---
### 17/05/22
***Which function is she using?***

Kiara was leaving her team, but she didn't want to go without having some fun.

She put together a simple neural network with one hidden layer. Never trained it, but she initialized its parameters and told her team that they should expect every node from the hidden layer to return a value resembling the following formula:

y = max(0.01 * x, 0)

Kiara saved the model and asked her team to test the node results without looking at the code. They found out that, in effect, the results always followed the formula mentioned by Kiara.

Kiara's question to her team was simple:

**Which of the following activation functions am I using in this network?**

- [ ] Kiara is using the sigmoid activation function.
- [x] Kiara is using the Rectified Linear Unit activation function.
- [ ] Kiara is using the Leaky Rectified Linear Unit activation function.
- [ ] None of the above activation functions can produce this output. 
---
### 18/05/22
***Riley's speed-dating match***

If you spend all day sitting at a desk, you can't expect to have many opportunities to meet interesting people.

Riley decided to get to bull by the horns and checked in on one of those speed-dating sites that promise to find your perfect match.

But of course, Silicon Valley is a ridiculous caricature of the impossible, and Riley's first match decided to start blabbing about machine learning and dimensionality reduction algorithms. And if this wasn't crazy enough, Riley didn't think this person knew what he was talking about.

**Can you guess all the possible statements about dimensionality reduction that would make Riley's match incorrect?**

- [ ] Supervised learning algorithms can be used as dimensionality reduction techniques.
- [x] Every dimensionality reduction technique is a clustering technique, but every clustering technique is not a dimensionality reduction algorithm.
- [ ] Dimensionality reduction algorithms are primarily considered unsupervised learning techniques.
- [x] Nowadays, the most successful dimensionality reduction techniques are deep learning algorithms.
---
### 19/05/22
***Occam's Razor showoff***

Tiara's manager was a showoff. No matter the situation, he always found a way to show everyone how smart he was.

Tiara noticed that he's been getting into machine learning lately, and as cringe as it sounds, he has been using "Occam's Razor" on every occasion, even incorrectly. Tiara started a secret list collecting every scenario when her manager used Occam's Razor to explain a situation. At the end of the week, she sent it to many of her friends to have a good laugh.

**Which of the following situations from Tiara's list are you comfortable justifying with Occam's Razor?**
- [x] We should prefer simpler models with fewer coefficients over complex models like ensembles.
- [x] Feature selection and dimensionality reduction help simplify models to get better results. 
- [ ] Keeping the training process as fast as possible avoids overtraining and prevents overcomplicated results.
- [ ] Starting the training of the model using sensible values for the hyperparameters.
---
